import shutil
from asyncio import subprocess

import requests
import json
import os
import sqlite3
import re
import urllib3
import logging
import time
import argparse
import tempfile
import threading
from datetime import datetime
from dataclasses import asdict
from typing import List, Optional, Dict, Any
from contextlib import contextmanager
from video_downloader.download.manager import DownloadManager
from data.models.DataModels import *
from data.models.FeedModels import Feed, FeedVideoItem
from video_downloader.core.logger import LoggerManager, info, warning, error

# ç¦ç”¨urllib3çš„ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
logger_manager = LoggerManager()
# è®¾ç½®å…¨å±€æ—¥å¿—çº§åˆ«ä¸ºINFO
logger_manager.set_level("default", logging.INFO)


temp_dir = tempfile.mkdtemp(prefix="video_download_")
# # ç”¨äºæµ‹è¯•ç‰¹å®šæ ‡é¢˜æ ¼å¼çš„å‡½æ•°
# def test_title_extraction(text: str):
#     """
#     æµ‹è¯•ç‰¹å®šæ ¼å¼æ ‡é¢˜çš„æå–åŠŸèƒ½
#     """
#     # æµ‹è¯•ç”¨æˆ·æä¾›çš„ç‰¹å®šæ ‡é¢˜æ ¼å¼
#     test_description = text

#     print("\n=== æµ‹è¯•ç‰¹å®šæ ‡é¢˜æå– ===")
#     print(f"åŸå§‹æè¿°: {test_description}")
#     extracted_title = extract_title_from_description(test_description)
#     print(f"æå–çš„æ ‡é¢˜: {extracted_title}")
#     print(f"æ ‡é¢˜æå–æˆåŠŸ: {extracted_title != 'æœªè·å–åˆ°æ ‡é¢˜ä¿¡æ¯'}")
#     print("=== æµ‹è¯•ç»“æŸ ===\n")

# # ç”¨äºæµ‹è¯•URLåˆæˆåŠŸèƒ½çš„å‡½æ•°
# def test_url_composition():
#     # æµ‹è¯•ç”¨ä¾‹1: URLä¸ºç©ºï¼Œä½¿ç”¨UIDåˆæˆURL
#     test_data1 = {
#         'uid': 'test_uid_123',
#         'url': '',
#         'title': 'æµ‹è¯•è§†é¢‘1',
#         'description': 'è¿™æ˜¯æµ‹è¯•è§†é¢‘1çš„æè¿°'
#     }

#     # è¿‡æ»¤æ‰ä¸åœ¨ç±»å­—æ®µä¸­çš„é”®
#     field_names = {f.name for f in VideoRecord.__dataclass_fields__.values()}
#     filtered_data1 = {k: v for k, v in test_data1.items() if k in field_names}

#     # å¤„ç†URLä¸ºç©ºçš„æƒ…å†µ
#     url1 = filtered_data1.get('url', '')
#     uid1 = filtered_data1.get('uid', '')
#     if not url1 and uid1:
#         filtered_data1['url'] = f"https://videodelivery.net/{uid1}/manifest/video.m3u8"

#     print(f"æµ‹è¯•ç”¨ä¾‹1: URLä¸ºç©ºæ—¶åˆæˆURL - {filtered_data1['url']}")
#     assert filtered_data1['url'] == "https://videodelivery.net/test_uid_123/manifest/video.m3u8", "URLåˆæˆå¤±è´¥"

#     # æµ‹è¯•ç”¨ä¾‹2: URLå­˜åœ¨ï¼Œä¸åˆæˆURL
#     test_data2 = {
#         'uid': 'test_uid_456',
#         'url': 'https://existing.url/video.mp4',
#         'title': 'æµ‹è¯•è§†é¢‘2',
#         'description': 'è¿™æ˜¯æµ‹è¯•è§†é¢‘2çš„æè¿°'
#     }

#     # è¿‡æ»¤æ‰ä¸åœ¨ç±»å­—æ®µä¸­çš„é”®
#     field_names = {f.name for f in VideoRecord.__dataclass_fields__.values()}
#     filtered_data2 = {k: v for k, v in test_data2.items() if k in field_names}

#     # å¤„ç†URLä¸ºç©ºçš„æƒ…å†µ
#     url2 = filtered_data2.get('url', '')
#     uid2 = filtered_data2.get('uid', '')
#     if not url2 and uid2:
#         filtered_data2['url'] = f"https://videodelivery.net/{uid2}/manifest/video.m3u8"

#     print(f"æµ‹è¯•ç”¨ä¾‹2: URLå­˜åœ¨æ—¶ä¸åˆæˆURL - {filtered_data2['url']}")
#     assert filtered_data2['url'] == "https://existing.url/video.mp4", "ä¸åº”è¯¥åˆæˆURL"

#     print("URLåˆæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")


# def test_description_cleanup():
#     print("\n=== æµ‹è¯•descriptionæ¸…ç†åŠŸèƒ½ ===")
#     # æµ‹è¯•ç”¨ä¾‹1: åŒ…å«æ ‡é¢˜å’Œå¤šä¸ªæ ‡ç­¾çš„æƒ…å†µ
#     description1 = "ã€0722-28ã€‘å°æ¼”è¦ªèº«ç¤ºç¯„æ•™å¤§ç•è¡¨æ¼”å‹¾å¼•ç”·äººğŸ¤£ğŸ’™å¹•å¾ŒèŠ±çµ® é€†æ„› Revenged Love #é€†æ„› #æŸ´é›è›‹ #å³æ‰€ç• #æ¢“æ¸ #æ± é¨ #ç”°æ ©å¯§ #RevengedLove #ZiYu #WuSuowei #TianXuning #ChiCheng  #Memefans #BL #boyslove"

#     # æ¨¡æ‹Ÿå¤„ç†description - åªä¿ç•™æ ‡ç­¾å†…å®¹
#     if description1:
#         # åŒ¹é…æ‰€æœ‰æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´çš„å•è¯ï¼‰
#         hashtag_pattern = r'(#\S+)'
#         hashtags = re.findall(hashtag_pattern, description1)

#         if hashtags:
#             # ç»„åˆæ‰€æœ‰æ ‡ç­¾ï¼Œç”¨ç©ºæ ¼åˆ†éš”
#             cleaned_description1 = ' '.join(hashtags)
#         else:
#             cleaned_description1 = description1
#     else:
#         cleaned_description1 = description1

#     print(f"æµ‹è¯•ç”¨ä¾‹1: åŒ…å«æ ‡é¢˜å’Œå¤šä¸ªæ ‡ç­¾ - æ¸…ç†å: '{cleaned_description1}'")
#     expected_result1 = "#é€†æ„› #æŸ´é›è›‹ #å³æ‰€ç• #æ¢“æ¸ #æ± é¨ #ç”°æ ©å¯§ #RevengedLove #ZiYu #WuSuowei #TianXuning #ChiCheng #Memefans #BL #boyslove"
#     assert cleaned_description1 == expected_result1, "åŒ…å«æ ‡é¢˜å’Œå¤šä¸ªæ ‡ç­¾çš„æƒ…å†µå¤„ç†å¤±è´¥"

#     # æµ‹è¯•ç”¨ä¾‹2: åªæœ‰æ ‡ç­¾çš„æƒ…å†µ
#     description2 = "#é€†æ„› #æŸ´é›è›‹ #éƒ­åŸå®‡ åªæœ‰æ ‡ç­¾æ²¡æœ‰å…¶ä»–å†…å®¹"

#     # æ¨¡æ‹Ÿå¤„ç†description - åªä¿ç•™æ ‡ç­¾å†…å®¹
#     if description2:
#         # åŒ¹é…æ‰€æœ‰æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´çš„å•è¯ï¼‰
#         hashtag_pattern = r'(#\S+)'
#         hashtags = re.findall(hashtag_pattern, description2)

#         if hashtags:
#             # ç»„åˆæ‰€æœ‰æ ‡ç­¾ï¼Œç”¨ç©ºæ ¼åˆ†éš”
#             cleaned_description2 = ' '.join(hashtags)
#         else:
#             cleaned_description2 = description2
#     else:
#         cleaned_description2 = description2

#     print(f"æµ‹è¯•ç”¨ä¾‹2: åªæœ‰æ ‡ç­¾çš„æƒ…å†µ - æ¸…ç†å: '{cleaned_description2}'")
#     expected_result2 = "#é€†æ„› #æŸ´é›è›‹ #éƒ­åŸå®‡"
#     assert cleaned_description2 == expected_result2, "åªæœ‰æ ‡ç­¾çš„æƒ…å†µå¤„ç†å¤±è´¥"

#     # æµ‹è¯•ç”¨ä¾‹3: æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µ
#     description3 = "æ²¡æœ‰æ ‡ç­¾åªæœ‰æ™®é€šæè¿°å†…å®¹"

#     # æ¨¡æ‹Ÿå¤„ç†description - åªä¿ç•™æ ‡ç­¾å†…å®¹
#     if description3:
#         # åŒ¹é…æ‰€æœ‰æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´çš„å•è¯ï¼‰
#         hashtag_pattern = r'(#\S+)'
#         hashtags = re.findall(hashtag_pattern, description3)

#         if hashtags:
#             # ç»„åˆæ‰€æœ‰æ ‡ç­¾ï¼Œç”¨ç©ºæ ¼åˆ†éš”
#             cleaned_description3 = ' '.join(hashtags)
#         else:
#             cleaned_description3 = description3
#     else:
#         cleaned_description3 = description3

#     print(f"æµ‹è¯•ç”¨ä¾‹3: æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µ - æ¸…ç†å: '{cleaned_description3}'")
#     expected_result3 = "æ²¡æœ‰æ ‡ç­¾åªæœ‰æ™®é€šæè¿°å†…å®¹"
#     assert cleaned_description3 == expected_result3, "æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µå¤„ç†å¤±è´¥"

#     print("descriptionåªä¿ç•™æ ‡ç­¾å†…å®¹åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")


class SimpleDatabaseManager:
    """
    ç®€å•çš„æ•°æ®åº“ç®¡ç†å™¨ - ä¸ä¾èµ–é¡¹ç›®ç°æˆä»£ç ï¼Œå®ç°åŸºæœ¬çš„æ•°æ®åº“æ“ä½œ
    """

    def __init__(self, db_path: str = "data/simple_m3u8.db"):
        self.db_path = db_path
        self.init_database()

    @contextmanager
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = None
        try:
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(os.path.abspath(self.db_path)), exist_ok=True)

            # è¿æ¥æ•°æ®åº“
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row  # ä½¿æŸ¥è¯¢ç»“æœå¯ä»¥é€šè¿‡åˆ—åè®¿é—®
            yield conn
        except sqlite3.Error as e:
            error(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                conn.close()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with self.get_connection() as conn:
            cursor = conn.cursor()

            # åˆ›å»ºä½œè€…è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS authors (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    username TEXT NOT NULL,
                    avatar TEXT,
                    region TEXT,
                    created_at TEXT,
                    role TEXT,
                    status TEXT,
                    invitation_id TEXT
                )
            """)

            # åˆ›å»ºé›†åˆè¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS collections (
                    id TEXT PRIMARY KEY,
                    type TEXT,
                    status TEXT,
                    created_at TEXT,
                    updated_at TEXT,
                    region TEXT,
                    author_id TEXT,
                    title TEXT,
                    description TEXT,
                    cover TEXT,
                    original_cover TEXT,
                    subscriber_count INTEGER,
                    contributor_count INTEGER,
                    content_type TEXT,
                    contents_count INTEGER,
                    carnival_status TEXT,
                    carnival_start_time TEXT,
                    is_subscribed INTEGER,
                    is_post_in_collection INTEGER,
                    is_contributor INTEGER,
                    can_commit INTEGER,
                    chat_join_threshold INTEGER,
                    FOREIGN KEY (author_id) REFERENCES authors (id)
                )
            """)

            # åˆ›å»ºè§†é¢‘è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS videos (
                    id TEXT PRIMARY KEY,
                    type TEXT,
                    title TEXT NOT NULL,
                    cover TEXT,
                    url TEXT,
                    url_type TEXT,
                    description TEXT,
                    status TEXT,
                    created_at TEXT,
                    updated_at TEXT,
                    comments_count INTEGER,
                    likes_count INTEGER,
                    collections_count INTEGER,
                    processing_status TEXT,
                    region TEXT,
                    width INTEGER,
                    height INTEGER,
                    uid TEXT,
                    is_locked INTEGER,
                    holdview_amount TEXT,
                    free_seconds INTEGER,
                    author_id TEXT,
                    is_liked INTEGER,
                    is_in_collection INTEGER,
                    is_favorite INTEGER,
                    shoot_period TEXT DEFAULT '0000',
                    FOREIGN KEY (author_id) REFERENCES authors (id)
                )
            """)

            # åˆ›å»ºè§†é¢‘æ ‡ç­¾è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS video_tags (
                    video_id TEXT,
                    tag TEXT,
                    PRIMARY KEY (video_id, tag),
                    FOREIGN KEY (video_id) REFERENCES videos (id) ON DELETE CASCADE
                )
            """)

            # åˆ›å»ºé›†åˆè§†é¢‘å…³è”è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS collection_videos (
                    collection_id TEXT,
                    video_id TEXT,
                    sequence INTEGER,
                    PRIMARY KEY (collection_id, video_id),
                    FOREIGN KEY (collection_id) REFERENCES collections (id) ON DELETE CASCADE,
                    FOREIGN KEY (video_id) REFERENCES videos (id) ON DELETE CASCADE
                )
            """)

            # åˆ›å»ºè®¢é˜…è¡¨ - ä¿®æ­£åçš„ç»“æ„ï¼ŒåŒ…å«åˆ†é¡µä¿¡æ¯
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS feeds (
                    id TEXT PRIMARY KEY DEFAULT 'default_feed',
                    contents_count INTEGER,
                    total INTEGER,
                    page INTEGER,
                    size INTEGER,
                    pages INTEGER
                )
            """)

            # åˆ›å»ºè®¢é˜…è§†é¢‘å…³è”è¡¨
            (
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS feed_videos (
                    feed_id TEXT,
                    video_id TEXT,
                    sequence INTEGER,
                    PRIMARY KEY (feed_id, video_id),
                    FOREIGN KEY (feed_id) REFERENCES feeds (id) ON DELETE CASCADE,
                    FOREIGN KEY (video_id) REFERENCES videos (id) ON DELETE CASCADE
                )
            """),
            )

            conn.commit()
            info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")

    def save_author(self, author: "Author"):
        """ä¿å­˜ä½œè€…ä¿¡æ¯åˆ°æ•°æ®åº“"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                # æ’å…¥æˆ–æ›¿æ¢ä½œè€…æ•°æ®
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO authors (
                        id, name, username, avatar, region, created_at, role, status, invitation_id
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        author.id,
                        author.name,
                        author.username,
                        author.avatar,
                        author.region,
                        author.created_at,
                        author.role,
                        author.status,
                        author.invitation_id,
                    ),
                )
                conn.commit()
                return True
            except sqlite3.Error as e:
                error(f"ä¿å­˜ä½œè€…ä¿¡æ¯å¤±è´¥: {e}")
                return False

    def save_collection(self, collection: "CollectionData"):
        """ä¿å­˜é›†åˆæ•°æ®åˆ°æ•°æ®åº“"""
        # å…ˆä¿å­˜ä½œè€…ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if collection.author:
            self.save_author(collection.author)

        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                # æ’å…¥æˆ–æ›¿æ¢é›†åˆæ•°æ®
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO collections (
                        id, type, status, created_at, updated_at, region, author_id, title,
                        description, cover, original_cover, subscriber_count, contributor_count,
                        content_type, contents_count, carnival_status, carnival_start_time,
                        is_subscribed, is_post_in_collection, is_contributor, can_commit,
                        chat_join_threshold
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        collection.id,
                        collection.type,
                        collection.status,
                        collection.created_at,
                        collection.updated_at,
                        collection.region,
                        collection.author_id,
                        collection.title,
                        collection.description,
                        collection.cover,
                        collection.original_cover,
                        collection.subscriber_count,
                        collection.contributor_count,
                        collection.content_type,
                        collection.contents_count,
                        collection.carnival_status,
                        collection.carnival_start_time,
                        1
                        if collection.is_subscribed
                        else 0
                        if collection.is_subscribed is not None
                        else None,
                        1
                        if collection.is_post_in_collection
                        else 0
                        if collection.is_post_in_collection is not None
                        else None,
                        1
                        if collection.is_contributor
                        else 0
                        if collection.is_contributor is not None
                        else None,
                        1
                        if collection.can_commit
                        else 0
                        if collection.can_commit is not None
                        else None,
                        collection.chat_join_threshold,
                    ),
                )
                conn.commit()
                return True
            except sqlite3.Error as e:
                error(f"ä¿å­˜é›†åˆæ•°æ®å¤±è´¥: {e}")
                return False

    def save_video(self, video: "VideoRecord"):
        """ä¿å­˜è§†é¢‘è®°å½•åˆ°æ•°æ®åº“"""
        # å…ˆä¿å­˜ä½œè€…ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if video.author:
            self.save_author(video.author)

        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                # å¼€å§‹äº‹åŠ¡
                conn.execute("BEGIN TRANSACTION")

                # æ’å…¥æˆ–æ›¿æ¢è§†é¢‘æ•°æ®
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO videos (
                        id, type, title, cover, url, url_type, description, status,
                        created_at, updated_at, comments_count, likes_count, collections_count,
                        processing_status, region, width, height, uid, is_locked,
                        holdview_amount, free_seconds, author_id, is_liked,
                        is_in_collection, is_favorite, shoot_period
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        video.id,
                        video.type,
                        video.title,
                        video.cover,
                        video.url,
                        video.url_type,
                        video.description,
                        video.status,
                        video.created_at,
                        video.updated_at,
                        video.comments_count,
                        video.likes_count,
                        video.collections_count,
                        video.processing_status,
                        video.region,
                        video.width,
                        video.height,
                        video.uid,
                        1 if video.is_locked else 0,
                        video.holdview_amount,
                        video.free_seconds,
                        video.author.id if video.author else None,
                        1 if video.is_liked else 0,
                        1 if video.is_in_collection else 0,
                        1 if video.is_favorite else 0,
                        video.shoot_period,
                    ),
                )

                # ä¿å­˜æ ‡ç­¾
                # å…ˆåˆ é™¤æ—§æ ‡ç­¾
                cursor.execute("DELETE FROM video_tags WHERE video_id = ?", (video.id,))
                # æ’å…¥æ–°æ ‡ç­¾
                for tag in video.tags:
                    cursor.execute(
                        "INSERT OR REPLACE INTO video_tags (video_id, tag) VALUES (?, ?)",
                        (video.id, tag),
                    )

                # æäº¤äº‹åŠ¡
                conn.commit()
                return True
            except sqlite3.Error as e:
                error(f"ä¿å­˜è§†é¢‘æ•°æ®å¤±è´¥: {e}")
                conn.rollback()
                return False

    def save_collection_videos(self, collection_id: str, video_ids: List[str]):
        """ä¿å­˜é›†åˆä¸è§†é¢‘çš„å…³è”å…³ç³»"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                # å¼€å§‹äº‹åŠ¡
                conn.execute("BEGIN TRANSACTION")

                # å…ˆåˆ é™¤æ—§çš„å…³è”å…³ç³»
                cursor.execute(
                    "DELETE FROM collection_videos WHERE collection_id = ?",
                    (collection_id,),
                )

                # æ’å…¥æ–°çš„å…³è”å…³ç³»
                for idx, video_id in enumerate(video_ids):
                    cursor.execute(
                        "INSERT OR REPLACE INTO collection_videos (collection_id, video_id, sequence) VALUES (?, ?, ?)",
                        (collection_id, video_id, idx),
                    )

                # æäº¤äº‹åŠ¡
                conn.commit()
                return True
            except sqlite3.Error as e:
                error(f"ä¿å­˜é›†åˆè§†é¢‘å…³è”å¤±è´¥: {e}")
                conn.rollback()
                return False

    def get_collection_videos_ids(self, collection_id: str) -> set:
        """ä»æ•°æ®åº“è·å–é›†åˆçš„è§†é¢‘IDåˆ—è¡¨"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "SELECT video_id FROM collection_videos WHERE collection_id = ? ORDER BY sequence",
                    (collection_id,),
                )
                results = cursor.fetchall()
                # è¿”å›è§†é¢‘IDçš„é›†åˆ
                return {row["video_id"] for row in results}
            except sqlite3.Error as e:
                error(f"è·å–é›†åˆè§†é¢‘IDå¤±è´¥: {e}")
                return set()

    def save_feed(self, feed: "Feed"):
        """ä¿å­˜è®¢é˜…ä¿¡æ¯åˆ°æ•°æ®åº“"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                # æ’å…¥æˆ–æ›¿æ¢è®¢é˜…æ•°æ®
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO feeds (
                        id, contents_count, total, page, size, pages
                    ) VALUES (?, ?, ?, ?, ?, ?)
                """,
                    (
                        "default_feed",  # å›ºå®šä½¿ç”¨'default_feed'ä½œä¸ºID
                        len(feed.items),
                        feed.total,
                        feed.page,
                        feed.size,
                        feed.pages,
                    ),
                )
                conn.commit()
                return True
            except sqlite3.Error as e:
                error(f"ä¿å­˜è®¢é˜…ä¿¡æ¯å¤±è´¥: {e}")
                return False

    def save_feed_videos(self, feed_id: str, feed_items: List["FeedVideoItem"]):
        """ä¿å­˜è®¢é˜…ä¸è§†é¢‘çš„å…³è”å…³ç³»"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                # å¼€å§‹äº‹åŠ¡
                conn.execute("BEGIN TRANSACTION")

                # å…ˆåˆ é™¤æ—§çš„å…³è”å…³ç³»
                cursor.execute("DELETE FROM feed_videos WHERE feed_id = ?", (feed_id,))

                # æ’å…¥æ–°çš„å…³è”å…³ç³»
                for idx, item in enumerate(feed_items):
                    cursor.execute(
                        "INSERT OR REPLACE INTO feed_videos (feed_id, video_id, sequence) VALUES (?, ?, ?)",
                        (feed_id, item.id, idx),
                    )

                # æäº¤äº‹åŠ¡
                conn.commit()
                return True
            except sqlite3.Error as e:
                error(f"ä¿å­˜è®¢é˜…è§†é¢‘å…³è”å¤±è´¥: {e}")
                conn.rollback()
                return False

    def get_feed_videos_ids(self, feed_id: str) -> set:
        """ä»æ•°æ®åº“è·å–è®¢é˜…çš„è§†é¢‘IDåˆ—è¡¨"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(
                    "SELECT video_id FROM feed_videos WHERE feed_id = ? ORDER BY sequence",
                    (feed_id,),
                )
                results = cursor.fetchall()
                # è¿”å›è§†é¢‘IDçš„é›†åˆ
                return {row["video_id"] for row in results}
            except sqlite3.Error as e:
                error(f"è·å–è®¢é˜…è§†é¢‘IDå¤±è´¥: {e}")
                return set()


def make_api_request(
    url: str,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    backoff_factor: float = 2.0,
) -> Optional[Dict[str, Any]]:
    """
    å‘æŒ‡å®šçš„API URLå‘é€GETè¯·æ±‚å¹¶è¿”å›å“åº”ç»“æœ

    Args:
        url (str): è¯·æ±‚çš„URLåœ°å€
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3æ¬¡
        retry_delay (float): åˆå§‹é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º1.0
        backoff_factor (float): å»¶è¿Ÿæ—¶é—´é€’å¢å› å­ï¼Œé»˜è®¤ä¸º2.0

    Returns:
        Optional[Dict[str, Any]]: APIè¿”å›çš„JSONæ•°æ®ï¼Œå¦‚æœè¯·æ±‚å¤±è´¥åˆ™è¿”å›None
    """

    config = Config()

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                delay = retry_delay * (backoff_factor ** (attempt - 1))
                warning(f"â³ APIè¯·æ±‚ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)

            info(f"ğŸ”„ APIè¯·æ±‚å°è¯• {attempt + 1}/{max_retries + 1}")

            # å‘é€GETè¯·æ±‚ï¼Œè·³è¿‡SSLè¯ä¹¦éªŒè¯
            response = requests.get(url, verify=False, headers=config.DEFAULT_HEADERS)

            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            if response.status_code == 200:
                # å°è¯•è§£æJSONå“åº”
                try:
                    data = response.json()
                    if attempt > 0:
                        info(f"âœ… APIè¯·æ±‚é‡è¯•æˆåŠŸï¼")
                    return data
                except json.JSONDecodeError:
                    error(f"è¯·æ±‚æˆåŠŸï¼Œä½†è¿”å›çš„æ•°æ®ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {url}")
                    error(f"å“åº”å†…å®¹ï¼š{response.text[:100]}...")
                    if attempt < max_retries:
                        warning(f"âš ï¸ å‡†å¤‡é‡è¯•...")
                        continue
                    return None
            else:
                error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼ŒURL: {url}")
                error(f"é”™è¯¯ä¿¡æ¯ï¼š{response.text[:100]}...")
                if attempt < max_retries:
                    warning(f"âš ï¸ å‡†å¤‡é‡è¯•...")
                    continue
                return None
        except requests.exceptions.RequestException as e:
            if attempt < max_retries:
                error(f"âŒ APIè¯·æ±‚å¼‚å¸¸: {e}ï¼ŒURL: {url}ï¼Œå‡†å¤‡é‡è¯•...")
            else:
                error(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡ï¼Œæœ€åé”™è¯¯: {e}")

    error(f"ğŸ’¥ APIè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
    return None


def extract_title_from_description(description: str) -> str:
    """
    ä»descriptionä¸­æå–è§†é¢‘æ ‡é¢˜
    å¤„ç†å¤šç§æ ‡é¢˜æ ¼å¼ï¼š
    - ã€0725-7ã€‘æ± é¨æŠ±å¤§ç•åšé«”èƒ½è¨“ç·´ğŸ˜‚ğŸ’™å¹•å¾ŒèŠ±çµ® é€†æ„› Revenged Love
    - ã€æœé…±0725-7ã€‘æ± é¨æŠ±å¤§ç•åšé«”èƒ½è¨“ç·´ğŸ˜‚ğŸ’™å¹•å¾ŒèŠ±çµ® é€†æ„› Revenged Love
    - ã€Revenged Loveã€‘0623 BTS Collection | é€†æ„› 0623 èŠ±çµ®åˆé›†
    - ã€Revenged Loveã€‘0701 BTS Collection | é€†æ„› 0701 èŠ±çµ®åˆé›†
    """
    if not description:
        return "æœªè·å–åˆ°æ ‡é¢˜ä¿¡æ¯"

    # æ–¹æ³•1: é¦–å…ˆå°è¯•ç›´æ¥æå–ã€ã€‘ä¸­çš„å†…å®¹ä½œä¸ºæ ‡é¢˜
    bracket_pattern = r"ã€([^ã€‘]+)ã€‘"
    match = re.search(bracket_pattern, description)
    if match:
        bracket_content = match.group(1).strip()
        # å®Œæ•´æå–åŒ…æ‹¬ã€ã€‘çš„æ ‡é¢˜
        full_title = match.group(0).strip()

        # æŸ¥æ‰¾ã€ã€‘åé¢å¯èƒ½çš„å†…å®¹ï¼Œç›´åˆ°é‡åˆ°æ ‡ç­¾æˆ–ç»“æŸ
        after_bracket = description[match.end() :].strip()
        if after_bracket:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ ‡ç­¾çš„ä½ç½®
            tag_match = re.search(r"#\w+", after_bracket)
            if tag_match:
                # æå–ã€ã€‘å’Œæ ‡ç­¾ä¹‹é—´çš„å†…å®¹
                between_content = after_bracket[: tag_match.start()].strip()
                if between_content:
                    return full_title + between_content
            else:
                # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œè¿”å›ã€ã€‘å†…å®¹åŠ ä¸Šåé¢çš„æ‰€æœ‰å†…å®¹ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                combined_title = full_title + after_bracket
                return combined_title[:150]  # é™åˆ¶æ ‡é¢˜é•¿åº¦

        return full_title

    # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡ç­¾éƒ¨åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼Œæ›´çµæ´»åœ°åŒ¹é…æ ‡ç­¾ï¼‰
    # åŒ¹é…ä»»ä½•ä»¥#å¼€å¤´çš„æ ‡ç­¾ï¼ŒåŒ…æ‹¬å«æœ‰ç©ºæ ¼çš„è‹±æ–‡æ ‡ç­¾
    hashtag_pattern = (
        r"#é€†æ„›|#æŸ´é›è›‹|#éƒ­åŸå®‡|#å±•è»’|#å§œå°å¸…|#åˆ˜è½©ä¸|#Revenged\s*Love|#BTS|#èŠ±çµ®|#åˆé›†"
    )
    match = re.search(hashtag_pattern, description, re.IGNORECASE)

    if match:
        # æå–æ ‡ç­¾å‰çš„éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
        title_part = description[: match.start()].strip()
        if title_part:
            return title_part

    # æ–¹æ³•3: å°è¯•æå–å¼€å¤´åˆ°ç¬¬ä¸€ä¸ª|ç¬¦å·çš„å†…å®¹
    pipe_pattern = r"^([^|]+?)\s*\|"
    match = re.search(pipe_pattern, description)
    if match:
        title_part = match.group(1).strip()
        if title_part:
            return title_part

    # æ–¹æ³•4: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹æ®Šæ ¼å¼çš„æ ‡é¢˜ï¼Œè¿”å›å‰100ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜ï¼ˆæ’é™¤æ˜æ˜¾çš„æ ‡ç­¾å†…å®¹ï¼‰
    # ç§»é™¤æ‰€æœ‰æ ‡ç­¾
    clean_description = re.sub(r"#\w+", "", description)
    clean_description = clean_description.strip()

    if clean_description:
        return clean_description[:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦

    return "æœªè·å–åˆ°æ ‡é¢˜ä¿¡æ¯"


def extract_shoot_period(title: str) -> str:
    """
    ä»æ ‡é¢˜ä¸­æå–è§†é¢‘çš„æ‹æ‘„æ—¶æœŸ
    æ”¯æŒçš„æ ¼å¼ï¼š
    - ã€0715-2ã€‘å°å¸¥æŠŠéƒ­åŸå®‡è¿·å¾—å¿˜äº†è©ğŸ’™å¹•å¾ŒèŠ±çµ® é€†æ„› Revenged Love -> 0715
    - ã€æœé…±0725-7ã€‘æ± é¨æŠ±å¤§ç•åšé«”èƒ½è¨“ç·´ğŸ˜‚ğŸ’™å¹•å¾ŒèŠ±çµ® -> 0725
    - ã€Revenged Loveã€‘0623 BTS Collection -> 0623
    - å¯¹äºæ— æ³•æå–çš„æƒ…å†µï¼Œè¿”å›'0000'
    """
    if not title:
        return "0000"

    # æ¨¡å¼1ï¼šã€0715-2ã€‘æ ¼å¼ï¼Œæå–ã€ã€‘ä¸­çš„æ•°å­—éƒ¨åˆ†
    pattern1 = r"ã€(\d{4})[-_]\d+ã€‘"
    match = re.search(pattern1, title)
    if match:
        return match.group(1)

    # æ¨¡å¼2ï¼šã€æœé…±0725-7ã€‘æ ¼å¼ï¼Œæå–ã€ã€‘ä¸­çš„æ•°å­—éƒ¨åˆ†
    pattern2 = r"ã€.*?(\d{4})[-_]\d+ã€‘"
    match = re.search(pattern2, title)
    if match:
        return match.group(1)

    # æ¨¡å¼3ï¼šã€Revenged Loveã€‘0623 æ ¼å¼ï¼Œæå–åé¢çš„æ•°å­—éƒ¨åˆ†
    pattern3 = r"ã€.*?ã€‘\s*(\d{4})"
    match = re.search(pattern3, title)
    if match:
        return match.group(1)

    # æ¨¡å¼4ï¼šæ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦æœ‰å•ç‹¬çš„4ä½æ•°å­—ï¼ˆå¯èƒ½æ˜¯æ—¥æœŸï¼‰
    pattern4 = r"\b(\d{4})\b"
    matches = re.finditer(pattern4, title)
    for match in matches:
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæœˆä»½å’Œæ—¥æœŸçš„èŒƒå›´ï¼ˆ01-12æœˆï¼Œ01-31æ—¥ï¼‰
        digits = match.group(1)
        if len(digits) == 4:
            month = int(digits[:2])
            day = int(digits[2:])
            if 1 <= month <= 12 and 1 <= day <= 31:
                return digits

    # å¦‚æœæ— æ³•æå–ï¼Œè¿”å›é»˜è®¤å€¼
    return "0000"


def get_video_record(video_id: str) -> Optional[VideoRecord]:
    """
    è·å–å•ä¸ªè§†é¢‘æ•°æ®å¹¶åˆ›å»ºVideoRecordå®ä¾‹
    """
    url = f"https://api.memefans.ai/v2/posts/videos/{video_id}"
    info(f"æ­£åœ¨è¯·æ±‚è§†é¢‘æ•°æ®: {url}")

    # å‘é€APIè¯·æ±‚
    data = make_api_request(url)

    # print(f"è¯·æ±‚åˆ°çš„è§†é¢‘æ•°æ®: {data}")

    if data:
        try:
            # å¤„ç†åµŒå¥—çš„authorå¯¹è±¡
            author_data = data.get("author")
            author = Author(**author_data) if author_data else None

            # åˆ›å»ºæ•°æ®å‰¯æœ¬å¹¶å¤„ç†authorå­—æ®µ
            data_copy = data.copy()
            if author_data:
                data_copy["author"] = author

            # å¤„ç†æ ‡é¢˜ä¸ºç©ºçš„æƒ…å†µ
            title = data_copy.get("title", "")
            description = data_copy.get("description", "")

            if not title or title.strip() == "":
                warning(f"è§†é¢‘ID: {video_id} çš„æ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»descriptionä¸­æå–")
                # ä»descriptionä¸­æå–æ ‡é¢˜
                extracted_title = extract_title_from_description(description)
                data_copy["title"] = extracted_title
                info(f"æˆåŠŸä»descriptionä¸­æå–æ ‡é¢˜: {extracted_title}")
            elif description and title in description:
                # å¦‚æœdescriptionä¸­åŒ…å«å®Œæ•´çš„æ ‡é¢˜ï¼Œä¹Ÿå°è¯•æå–æ›´å®Œæ•´çš„æ ‡é¢˜
                info(
                    f"è§†é¢‘ID: {video_id} çš„æ ‡é¢˜å¯èƒ½ä¸å®Œæ•´ï¼Œå°è¯•ä»descriptionä¸­æå–æ›´å®Œæ•´çš„æ ‡é¢˜"
                )
                extracted_title = extract_title_from_description(description)
                if (
                    extracted_title
                    and extracted_title != "æœªè·å–åˆ°æ ‡é¢˜ä¿¡æ¯"
                    and len(extracted_title) > len(title)
                ):
                    data_copy["title"] = extracted_title
                    info(f"æˆåŠŸä»descriptionä¸­æå–æ›´å®Œæ•´çš„æ ‡é¢˜: {extracted_title}")

            # ä»æ ‡é¢˜ä¸­æå–æ‹æ‘„æ—¶æœŸ
            current_title = data_copy.get("title", "")
            shoot_period = extract_shoot_period(current_title)
            data_copy["shoot_period"] = shoot_period
            info(f"è§†é¢‘ID: {video_id} çš„æ‹æ‘„æ—¶æœŸ: {shoot_period}")

            # ä»descriptionä¸­åªä¿ç•™æ ‡ç­¾å†…å®¹
            if description:
                # åŒ¹é…æ‰€æœ‰æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´çš„å•è¯ï¼‰
                hashtag_pattern = r"(#\S+)"
                hashtags = re.findall(hashtag_pattern, description)

                if hashtags:
                    # ç»„åˆæ‰€æœ‰æ ‡ç­¾ï¼Œç”¨ç©ºæ ¼åˆ†éš”
                    cleaned_description = " ".join(hashtags)
                    data_copy["description"] = cleaned_description
                    info(f"è§†é¢‘ID: {video_id} çš„descriptionå·²åªä¿ç•™æ ‡ç­¾å†…å®¹")

            # è¿‡æ»¤æ‰ä¸åœ¨ç±»å­—æ®µä¸­çš„é”®
            field_names = {f.name for f in VideoRecord.__dataclass_fields__.values()}
            filtered_data = {k: v for k, v in data_copy.items() if k in field_names}

            # å¤„ç†URLä¸ºç©ºçš„æƒ…å†µ
            url = filtered_data.get("url", "")
            uid = filtered_data.get("uid", "")
            if not url and uid:
                filtered_data["url"] = (
                    f"https://videodelivery.net/{uid}/manifest/video.m3u8"
                )
                info(
                    f"è§†é¢‘ID: {video_id} çš„URLä¸ºç©ºï¼Œä½¿ç”¨UIDåˆæˆURL: {filtered_data['url']}"
                )

            info(f"è§†é¢‘ID: {video_id} çš„è§†é¢‘é“¾æ¥: {url if url else 'Null'}")
            # ä»æ ‡é¢˜ä¸­æå–æ‹æ‘„æ—¥æœŸ
            video_date = extract_shoot_period(current_title)
            filtered_data["video_date"] = video_date
            # print(f"è§†é¢‘ID: {video_id} çš„æ‹æ‘„æ—¥æœŸ: {video_date}")

            # åˆ›å»ºå¹¶è¿”å›VideoRecordå®ä¾‹
            video_record = VideoRecord(**filtered_data)
            info(f"æˆåŠŸåˆ›å»ºVideoRecordå®ä¾‹: {video_id}")
            return video_record
        except Exception as e:
            error(f"åˆ›å»ºVideoRecordå®ä¾‹å¤±è´¥: {e}ï¼Œè§†é¢‘ID: {video_id}")
            return None

    return None


def show_live_countdown(seconds):
    try:
        # å°†printæ”¹ä¸ºinfoæ—¥å¿—
        info(f"ä¸‹ä¸€æ¬¡æ£€æŸ¥å°†åœ¨ {seconds} ç§’åè¿›è¡Œ")

        # æ˜¾ç¤ºå€’è®¡æ—¶
        for remaining in range(seconds, 0, -1):
            minutes, secs = divmod(remaining, 60)
            # ä½¿ç”¨å›è½¦ç¬¦è¦†ç›–å½“å‰è¡Œï¼Œå®ç°åŠ¨æ€æ›´æ–°æ•ˆæœ
            # å¯¹äºè¿™ç§å®æ—¶æ›´æ–°çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä»ä½¿ç”¨printä»¥ä¾¿åœ¨æ§åˆ¶å°æ›´å¥½åœ°æ˜¾ç¤º
            print(f"å‰©ä½™æ—¶é—´: {minutes}åˆ†{secs}ç§’", end="\r", flush=True)
            time.sleep(1)

        # å€’è®¡æ—¶ç»“æŸæ—¶æ¸…é™¤è¿™ä¸€è¡Œ
        print(" " * 50, end="\r", flush=True)
    except KeyboardInterrupt:
        # æ•è·é”®ç›˜ä¸­æ–­ä¿¡å·ï¼Œç¡®ä¿è¾“å‡ºæ­£ç¡®çš„çŠ¶æ€ä¿¡æ¯
        print(" " * 50, end="\r", flush=True)  # ä¿ç•™è¿™ä¸€è¡Œä½œä¸ºç»ˆç«¯äº¤äº’æ“ä½œ
        info("ç¨‹åºä¸­æ–­ä¸­...")
    except Exception as e:
        # å¦‚æœå‘ç”Ÿå…¶ä»–å¼‚å¸¸ï¼Œç¡®ä¿ä¸å½±å“ä¸»ç¨‹åº
        # ä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹è®°å½•é”™è¯¯ä¿¡æ¯
        # error(f"å€’è®¡æ—¶æ˜¾ç¤ºé”™è¯¯: {e}")
        pass


def show_latest_videos(db_manager):
    """
    æ˜¾ç¤ºæœ€æ–°çš„10æ¡è§†é¢‘ä¿¡æ¯ï¼ŒåŒ…æ‹¬IDã€æ ‡é¢˜å’Œé“¾æ¥å¯è®¿é—®çŠ¶æ€
    """
    try:
        info(f"ğŸ“¹ æœ€æ–°10æ¡è§†é¢‘ä¿¡æ¯ï¼ˆæŒ‰æ›´æ–°æ—¶é—´æ’åºï¼‰ï¼š")
        info(f"{'-' * 80}")
        info(f"{'ID':<20} {'æ ‡é¢˜':<40} {'é“¾æ¥çŠ¶æ€'}")
        info(f"{'-' * 80}")

        # ä»æ•°æ®åº“è·å–æœ€æ–°çš„10æ¡è§†é¢‘è®°å½•
        with db_manager.get_connection() as conn:
            cursor = conn.cursor()
            # æŸ¥è¯¢æœ€æ–°çš„10æ¡è§†é¢‘è®°å½•ï¼ŒæŒ‰updated_até™åºæ’åº
            cursor.execute(
                """SELECT id, title, url
                   FROM videos
                   ORDER BY updated_at DESC
                   LIMIT 20"""
            )
            results = cursor.fetchall()

            if results:
                for row in results:
                    video_id = row["id"]
                    title = row["title"]
                    url = row["url"]

                    # æ£€æŸ¥é“¾æ¥æ˜¯å¦å¯è®¿é—®ï¼ˆç®€åŒ–æ£€æŸ¥ï¼Œä»…åˆ¤æ–­URLæ˜¯å¦ä¸ºç©ºï¼‰
                    url_status = "âœ… å¯è®¿é—®" if url else "âŒ ä¸å¯è®¿é—®ï¼ˆä»˜è´¹è§†é¢‘ï¼‰"

                    # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜ä»¥ä¿æŒè¡¨æ ¼æ•´æ´
                    truncated_title = (title[:37] + "...") if len(title) > 40 else title

                    info(f"{video_id:<20} {truncated_title:<40} {url_status}")
                info(f"{'-' * 80}")
            else:
                info("æš‚æ— è§†é¢‘æ•°æ®")
    except Exception as e:
        error(f"æ˜¾ç¤ºæœ€æ–°è§†é¢‘ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# å®šæ—¶æ£€æŸ¥é›†åˆæ›´æ–°å¹¶ä¸‹è½½æ–°å¢è§†é¢‘çš„å‡½æ•°
def check_collection_updates(
    collection_url,
    db_manager,
    download_flag=False,
    interval=120,
    download_dir="./downloads",
):
    try:
        info(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹æ£€æŸ¥é›†åˆæ›´æ–°...")

        # æ˜¾ç¤ºæœ€æ–°10æ¡è§†é¢‘ä¿¡æ¯
        show_latest_videos(db_manager)

        # è¯·æ±‚æœ€æ–°çš„é›†åˆæ•°æ®
        data = make_api_request(collection_url)

        if data:
            # ä»JSONæ•°æ®åˆ›å»ºCollectionDataå®ä¾‹
            collection = CollectionData.from_json(data)

            # æå–å½“å‰é›†åˆçš„å†…å®¹IDé›†åˆ
            current_contents = set(collection.contents)

            # ä»æ•°æ®åº“è·å–ä¸Šæ¬¡ä¿å­˜çš„è§†é¢‘IDé›†åˆ
            saved_contents = db_manager.get_collection_videos_ids(collection.id)

            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ä¿å­˜è¿‡è¯¥é›†åˆçš„å†…å®¹ï¼Œæˆ–è€…é›†åˆå†…å®¹ä¸ºç©ºï¼Œä¿å­˜æ‰€æœ‰è§†é¢‘
            # if not saved_contents:
            info(f"é›†åˆåŒ…å« {len(current_contents)} ä¸ªè§†é¢‘")

            # ä¿å­˜é›†åˆæ•°æ®åˆ°æ•°æ®åº“
            if db_manager.save_collection(collection):
                info(f"é›†åˆæ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
            else:
                error(f"ä¿å­˜é›†åˆæ•°æ®åˆ°æ•°æ®åº“å¤±è´¥")

            # ä¿å­˜é›†åˆè§†é¢‘å…³è”å…³ç³»åˆ°æ•°æ®åº“
            if db_manager.save_collection_videos(collection.id, collection.contents):
                info(f"é›†åˆè§†é¢‘å…³è”å…³ç³»å·²ä¿å­˜åˆ°æ•°æ®åº“")
            else:
                error(f"ä¿å­˜é›†åˆè§†é¢‘å…³è”å…³ç³»åˆ°æ•°æ®åº“å¤±è´¥")

            # å¦‚æœå¯ç”¨äº†ä¸‹è½½åŠŸèƒ½ï¼Œä¸‹è½½æ‰€æœ‰è§†é¢‘
            process_and_download_videos(
                collection.contents[:5], db_manager, download_dir
            )
        else:
            error("è·å–é›†åˆæ•°æ®å¤±è´¥")
    except Exception as e:
        error(f"æ£€æŸ¥é›†åˆæ›´æ–°æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    try:
        # å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥æ˜¾ç¤ºå®æ—¶å€’è®¡æ—¶
        countdown_thread = threading.Thread(
            target=show_live_countdown, args=(interval,)
        )
        countdown_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œè¿™æ ·ä¸»çº¿ç¨‹ç»“æŸæ—¶å®ƒä¹Ÿä¼šç»“æŸ
        countdown_thread.start()

        # intervalç§’åå†æ¬¡æ‰§è¡Œ
        timer = threading.Timer(
            interval,
            check_feed_updates,
            args=[feed_url, db_manager, download_flag, interval],
        )
        timer.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
        timer.start()
    except Exception as e:
        # æ•è·çº¿ç¨‹åˆ›å»ºè¿‡ç¨‹ä¸­çš„å¼‚å¸¸
        error(f"åˆ›å»ºçº¿ç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def check_feed_updates(
    feed_url,
    db_manager: SimpleDatabaseManager,
    download_flag=False,
    interval=120,
    download_dir="./downloads",
    num=5,
    local_json_file="./temp/feed.json",
    page=1,
):
    try:
        info(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹æ£€æŸ¥è®¢é˜…æ›´æ–°...")

        # æ˜¾ç¤ºæœ€æ–°10æ¡è§†é¢‘ä¿¡æ¯
        show_latest_videos(db_manager)
        feed_url = f"{feed_url}?page={page}&size={num}"

        # è¯·æ±‚æœ€æ–°çš„è®¢é˜…æ•°æ®
        data = make_api_request(feed_url)
        # data = None

        # if not data:
        #     error("ä»APIå“åº”åˆ›å»ºFeedå®ä¾‹å¤±è´¥")
        #     try:
        #         feed= Feed.from_json_file(local_json_file)
        #     except Exception as e:
        #         error(f"ä»JSONæ–‡ä»¶åˆ›å»ºFeedå®ä¾‹å¤±è´¥: {e}")
        #         return

        if data:
            # ä»JSONæ•°æ®åˆ›å»ºFeedå®ä¾‹
            feed = Feed.from_api_response(data)

            # æå–å½“å‰è®¢é˜…çš„è§†é¢‘IDé›†åˆ
            current_contents = set([item.id for item in feed.items])

            # ä»æ•°æ®åº“è·å–ä¸Šæ¬¡ä¿å­˜çš„è§†é¢‘IDé›†åˆ
            # saved_contents = db_manager.get_feed_videos_ids('default_feed')

            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ä¿å­˜è¿‡è¯¥è®¢é˜…çš„å†…å®¹ï¼Œæˆ–è€…è®¢é˜…å†…å®¹ä¸ºç©ºï¼Œä¿å­˜æ‰€æœ‰è§†é¢‘
            # if not saved_contents:
            info(f"è®¢é˜…åŒ…å« {len(current_contents)} ä¸ªè§†é¢‘")
            info(
                f"åˆ†é¡µä¿¡æ¯: total={feed.total}, page={feed.page}, size={feed.size}, pages={feed.pages}"
            )

            # ä¿å­˜è®¢é˜…æ•°æ®åˆ°æ•°æ®åº“
            if db_manager.save_feed(feed):
                info(f"è®¢é˜…æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
            else:
                error(f"ä¿å­˜è®¢é˜…æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥")

            # ä¿å­˜è®¢é˜…è§†é¢‘å…³è”å…³ç³»åˆ°æ•°æ®åº“
            if db_manager.save_feed_videos("default_feed", feed.items):
                info(f"è®¢é˜…è§†é¢‘å…³è”å…³ç³»å·²ä¿å­˜åˆ°æ•°æ®åº“")
            else:
                error(f"ä¿å­˜è®¢é˜…è§†é¢‘å…³è”å…³ç³»åˆ°æ•°æ®åº“å¤±è´¥")

            # å¦‚æœå¯ç”¨äº†ä¸‹è½½åŠŸèƒ½ï¼Œä¸‹è½½æ‰€æœ‰è§†é¢‘
            process_and_download_videos(
                feed.filter_by_author_id("BhhLJPlVvjU")[:num], db_manager, download_dir
            )
        else:
            error("è·å–è®¢é˜…æ•°æ®å¤±è´¥")
    except Exception as e:
        error(f"æ£€æŸ¥è®¢é˜…æ›´æ–°æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    try:
        # å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥æ˜¾ç¤ºå®æ—¶å€’è®¡æ—¶
        countdown_thread = threading.Thread(
            target=show_live_countdown, args=(interval,)
        )
        countdown_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œè¿™æ ·ä¸»çº¿ç¨‹ç»“æŸæ—¶å®ƒä¹Ÿä¼šç»“æŸ
        countdown_thread.start()

        # intervalç§’åå†æ¬¡æ‰§è¡Œ
        timer = threading.Timer(
            interval,
            check_feed_updates,
            args=[feed_url, db_manager, download_flag, interval, download_dir, num],
        )
        timer.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
        timer.start()
    except Exception as e:
        # æ•è·çº¿ç¨‹åˆ›å»ºè¿‡ç¨‹ä¸­çš„å¼‚å¸¸
        error(f"åˆ›å»ºçº¿ç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# å¤„ç†å¹¶ä¸‹è½½è§†é¢‘çš„å‡½æ•°
def process_and_download_videos(
    videos: list[FeedVideoItem],
    db_manager: SimpleDatabaseManager,
    download_dir="./downloads",
):
    # åˆ›å»ºå­˜å‚¨JSONæ–‡ä»¶çš„ç›®å½•
    json_dir = "data/video_jsons"
    os.makedirs(json_dir, exist_ok=True)

    # åˆ›å»ºä¸‹è½½ç®¡ç†å™¨
    download_manager = DownloadManager()

    # å¤„ç†æ¯ä¸ªè§†é¢‘ID
    for video in videos:
        try:
            info(f"å¤„ç†è§†é¢‘ {video.id}")
            video_record = get_video_record(video.id)

            if video_record:
                # ä¿å­˜VideoRecordå®ä¾‹åˆ°æ–‡ä»¶
                video_file_path = f"{json_dir}/{video.id}.json"
                with open(video_file_path, "w", encoding="utf-8") as f:
                    json.dump(asdict(video_record), f, ensure_ascii=False, indent=2)
                info(f"è§†é¢‘æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{video_file_path}")

                # ä¿å­˜è§†é¢‘æ•°æ®åˆ°æ•°æ®åº“
                if db_manager.save_video(video_record):
                    info(f"è§†é¢‘æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
                else:
                    error(f"ä¿å­˜è§†é¢‘æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥")

                # æ¸…ç†æ ‡é¢˜ä½œä¸ºå®‰å…¨çš„æ–‡ä»¶å
                safe_title = download_manager.sanitize_filename(video_record.title)
                safe_date = download_manager.sanitize_filename(video_record.video_date)

                # æ„å»ºä¸‹è½½è·¯å¾„
                date_folder = os.path.join(download_dir, safe_date)
                # ä½¿ç”¨ä¸download_videoæ–¹æ³•ç›¸åŒçš„æ‰©å±•åæ ¼å¼ï¼Œç¡®ä¿è·¯å¾„åŒ¹é…
                output_filename = (
                    f"{safe_title}_{safe_date}.{download_manager.config.OUTPUT_FORMAT}"
                )
                output_path = os.path.join(date_folder, output_filename)

                # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ä»¥ç¡®ä¿ä¸€è‡´æ€§
                output_path_abs = os.path.abspath(output_path)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                file_exists = os.path.exists(output_path_abs)

                if file_exists:
                    info(f"ğŸ“ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {video_record.title}")
                    continue

                # ä¸‹è½½è§†é¢‘
                info(f"å¼€å§‹ä¸‹è½½è§†é¢‘: {video_record.title}")
                download_manager.download_video(video_record, download_dir)
                info(f"è§†é¢‘ä¸‹è½½å®Œæˆ: {video_record.title}")
        except Exception as e:
            error(f"å¤„ç†è§†é¢‘ {video.id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè§†é¢‘
            continue

    info(f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç†{len(videos)}ä¸ªè§†é¢‘")


def update_all_videos(db_manager: SimpleDatabaseManager, feed_url, download_dir="./downloads", pages=3):
    """
    æ›´æ–°æ‰€æœ‰è§†é¢‘æ•°æ®ï¼ŒåŒ…æ‹¬ä»APIè·å–æœ€æ–°æ•°æ®ã€ä¿å­˜åˆ°æ•°æ®åº“å’Œä¸‹è½½è§†é¢‘
    """
    try:
        for page in range(1, pages + 1):
            check_feed_updates(
                feed_url, db_manager, not args.no_download, args.interval, download_dir, num=50, page=page
            )


    except Exception as e:
        error(f"æ£€æŸ¥è®¢é˜…æ›´æ–°æ—¶å‘ç”Ÿé”™è¯¯: {e}")


# ä¸»å‡½æ•°å…¥å£
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="M3U8è§†é¢‘ä¸‹è½½å·¥å…·")
    parser.add_argument("--no-download", action="store_true", help="ç¦ç”¨è§†é¢‘ä¸‹è½½")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„è§†é¢‘")
    parser.add_argument("--dir", type=str, default="downloads", help="ä¸‹è½½ç›®å½•")
    parser.add_argument(
        "--interval",
        type=int,
        default=120,
        help="æ£€æŸ¥æ›´æ–°çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸º120ç§’ï¼ˆ2åˆ†é’Ÿï¼‰",
    )
    args = parser.parse_args()

    # è¿è¡Œæµ‹è¯•
    # print("\n=== è¿è¡ŒURLåˆæˆåŠŸèƒ½æµ‹è¯• ===")
    # test_url_composition()

    # è¿è¡Œdescriptionæ¸…ç†åŠŸèƒ½æµ‹è¯•
    # test_description_cleanup()

    # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
    db_manager = SimpleDatabaseManager()
    config = Config()
    # é›†åˆURL
    # collection_url = "https://api.memefans.ai/v2/posts/collections/BhhNPqDl_yW"
    feed_url = "https://api.memefans.ai/v2/feed"

    info(f"M3U8è§†é¢‘ä¸‹è½½å·¥å…·å¯åŠ¨")
    # print(f"é›†åˆURL: {collection_url}")
    info(f"è®¢é˜…URL: {feed_url}")
    info(f"æ£€æŸ¥æ›´æ–°é—´éš”: {args.interval}ç§’")
    info(f"ä¸‹è½½åŠŸèƒ½: {'ç¦ç”¨' if args.no_download else 'å¯ç”¨'}")
    info(f"æŒ‰Ctrl+Cåœæ­¢ç¨‹åº...")

    # data = make_api_request(collection_url)

    # if data:
    #     # ä»JSONæ•°æ®åˆ›å»ºCollectionDataå®ä¾‹
    #     collection = CollectionData.from_json(data)

    #     # æå–å½“å‰é›†åˆçš„å†…å®¹IDé›†åˆ
    #     current_contents = set(collection.contents)

    #     # ä»æ•°æ®åº“è·å–ä¸Šæ¬¡ä¿å­˜çš„è§†é¢‘IDé›†åˆ
    #     saved_contents = db_manager.get_collection_videos_ids(collection.id)

    #     print(f"é¦–æ¬¡è¿è¡Œæˆ–æ•°æ®åº“ä¸­æ²¡æœ‰ä¿å­˜é›†åˆå†…å®¹ï¼Œé›†åˆåŒ…å« {len(current_contents)} ä¸ªè§†é¢‘")

    #     # ä¿å­˜é›†åˆæ•°æ®åˆ°æ•°æ®åº“
    #     if db_manager.save_collection(collection):
    #         print(f"é›†åˆæ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
    #     else:
    #         print(f"ä¿å­˜é›†åˆæ•°æ®åˆ°æ•°æ®åº“å¤±è´¥")

    #     # ä¿å­˜é›†åˆè§†é¢‘å…³è”å…³ç³»åˆ°æ•°æ®åº“
    #     if db_manager.save_collection_videos(collection.id, collection.contents):
    #         print(f"é›†åˆè§†é¢‘å…³è”å…³ç³»å·²ä¿å­˜åˆ°æ•°æ®åº“")
    #     else:
    #         print(f"ä¿å­˜é›†åˆè§†é¢‘å…³è”å…³ç³»åˆ°æ•°æ®åº“å¤±è´¥")

    #     process_and_download_videos(collection.contents[:5], db_manager, "./downloads")

    # else:
    #     print("è·å–é›†åˆæ•°æ®å¤±è´¥")
    # update_all_videos(db_manager, feed_url,download_dir="./downloads",pages=3)

    # å¼€å§‹å®šæ—¶æ£€æŸ¥è®¢é˜…æ›´æ–°
    check_feed_updates(
        feed_url, db_manager, not args.no_download, args.interval, "./downloads", num=30, page=1
    )

    # ä¿æŒä¸»ç¨‹åºè¿è¡Œ
    try:
        while True:
            time.sleep(1)  # å‡å°‘ç¡çœ æ—¶é—´ï¼Œä½¿ç¨‹åºèƒ½å¤Ÿæ›´å¿«å“åº”ä¸­æ–­
    except KeyboardInterrupt:
        info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç¨‹åº...")
        # æ¸…é™¤å€’è®¡æ—¶æ˜¾ç¤ºè¡Œ
        print(" " * 50, end="\r", flush=True)  # ä¿ç•™è¿™ä¸€è¡Œä½œä¸ºç»ˆç«¯äº¤äº’æ“ä½œ
        # ç¡®ä¿è¾“å‡ºæ¢è¡Œï¼Œä½¿æœ€ç»ˆæ¶ˆæ¯æ˜¾ç¤ºåœ¨æ–°è¡Œ
        info("ç¨‹åºå·²å®‰å…¨åœæ­¢")
    except Exception as e:
        error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
